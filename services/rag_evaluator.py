"""
RAG Evaluation module using local Ollama-based evaluation to avoid OpenAI dependencies.

This module evaluates RAG system performance using ground truth QA pairs.
"""

import json
from typing import List, Dict, Any
from pathlib import Path
import numpy as np
import difflib
from langchain_community.llms import Ollama
from lightweight_embedder import LightweightEmbedder
from vector_store import QdrantVectorStore

class RAGEvaluator:
    """Evaluate RAG system using ground truth QA pairs."""

    def __init__(
        self,
        ground_truth_file: str,
        embedder=None,
        vector_store=None,
        qdrant_host: str = "host.docker.internal",
        collection_name: str = "eval_docs",
        ollama_base_url: str = "http://ollama:11434",
        ollama_model: str = "llama3.2:1b"
    ):
        """Initialize RAG evaluator.

        Args:
            ground_truth_file: Path to JSON file with ground truth QA pairs
            embedder: Optional embedder to use (uses LightweightEmbedder if None)
            vector_store: Optional vector store to use (uses QdrantVectorStore if None)
            qdrant_host: Host for Qdrant connection
            collection_name: Collection name for evaluation
            ollama_base_url: Base URL for Ollama service
            ollama_model: Model name to use with Ollama
        """
        self.ground_truth_path = Path(ground_truth_file)
        self.qa_pairs = self._load_ground_truth()

        # Configure Ollama for evaluation
        self.ollama_base_url = ollama_base_url
        self.ollama_model = ollama_model

        # Initialize Ollama LLM for local evaluation
        self.ollama_llm = Ollama(
            base_url=ollama_base_url,
            model=ollama_model,
            temperature=0.1
        )

        # Initialize embedder and vector store if not provided
        self.embedder = embedder or LightweightEmbedder(max_features=384)
        self.vector_store = vector_store or QdrantVectorStore(
            host=qdrant_host,
            collection_name=collection_name,
            vector_size=384
        )

    def _load_ground_truth(self) -> List[Dict[str, str]]:
        """Load ground truth QA pairs from JSON file."""
        with open(self.ground_truth_path, 'r') as f:
            data = json.load(f)
        return data

    def index_documents(self, documents: List[Dict[str, Any]]):
        """Index documents for evaluation.

        Args:
            documents: List of document dictionaries with 'content' and 'metadata'
        """
        texts = [doc["content"] for doc in documents]
        embeddings = self.embedder.embed(texts)

        payloads = []
        for i, doc in enumerate(documents):
            payload = {
                "text": doc["content"],
                "source": doc["metadata"].get("source", "unknown"),
                "category": doc["metadata"].get("category", "unknown")
            }
            payloads.append(payload)

        self.vector_store.upsert(embeddings, payloads)
        print(f"Indexed {len(documents)} documents for evaluation")

    def retrieve_context(self, query: str, top_k: int = 3) -> List[str]:
        """Retrieve context for a query.

        Args:
            query: Query string
            top_k: Number of documents to retrieve

        Returns:
            List of context passages
        """
        query_embedding = self.embedder.embed(query)[0]
        results = self.vector_store.search(query_embedding, limit=top_k)
        return [result["payload"]["text"] for result in results]

    def evaluate(self, questions: List[str], answers: List[str], model_name: str = "simple-model"):
        """Evaluate RAG system using local metrics (avoiding RAGAS OpenAI dependencies).

        Args:
            questions: List of questions
            answers: List of answers generated by the RAG system
            model_name: Name/identifier for the model being evaluated

        Returns:
            Dictionary with evaluation results
        """
        # Prepare data for evaluation
        retrieval_contexts = []
        ground_truths = []

        for i, question in enumerate(questions):
            # Retrieve context for each question
            contexts = self.retrieve_context(question)
            retrieval_contexts.append(contexts)

            # Find the corresponding ground truth
            found_pair = next((qa for qa in self.qa_pairs if qa["question"] == question), None)
            if found_pair:
                ground_truths.append(found_pair["answer"])
            else:
                ground_truths.append("No ground truth found")

        print(f"Evaluating {len(questions)} questions with local Ollama-based metrics...")

        # Use our enhanced local evaluation instead of RAGAS
        return self._enhanced_local_evaluation(questions, answers, retrieval_contexts, ground_truths, model_name)

    def _enhanced_local_evaluation(self, questions, answers, contexts, ground_truths, model_name):
        """Enhanced local evaluation using Ollama for semantic analysis."""
        print("Running enhanced local evaluation metrics...")

        relevance_scores = []
        faithfulness_scores = []
        context_precision_scores = []
        semantic_similarity_scores = []

        for i, (question, answer, context_list, ground_truth) in enumerate(zip(questions, answers, contexts, ground_truths)):
            print(f"Evaluating answer {i+1}...")

            # 1. Answer Relevancy (string similarity + semantic check)
            string_similarity = difflib.SequenceMatcher(None, answer.lower(), ground_truth.lower()).ratio()

            # Enhanced semantic similarity using Ollama
            semantic_score = self._evaluate_semantic_similarity(answer, ground_truth)
            combined_relevance = (string_similarity + semantic_score) / 2
            relevance_scores.append(combined_relevance)

            # 2. Faithfulness (how well answer sticks to context)
            context_text = " ".join(context_list).lower()
            answer_words = set(answer.lower().split())
            context_words = set(context_text.split())
            common_words = len(answer_words.intersection(context_words))
            faithfulness = min(1.0, common_words / max(1, len(answer_words)))

            # Enhanced faithfulness using Ollama
            ollama_faithfulness = self._evaluate_faithfulness(answer, context_list)
            combined_faithfulness = (faithfulness + ollama_faithfulness) / 2
            faithfulness_scores.append(combined_faithfulness)

            # 3. Context Precision (how relevant the retrieved contexts are)
            context_precision = self._evaluate_context_precision(question, context_list)
            context_precision_scores.append(context_precision)

            # 4. Semantic Similarity Score
            semantic_similarity_scores.append(semantic_score)

        return {
            "answer_relevancy": relevance_scores,
            "faithfulness": faithfulness_scores,
            "context_precision": context_precision_scores,
            "semantic_similarity": semantic_similarity_scores,
            "model": model_name
        }

    def _evaluate_semantic_similarity(self, answer: str, ground_truth: str) -> float:
        """Use Ollama to evaluate semantic similarity between answer and ground truth."""
        try:
            prompt = f"""Compare these two answers and rate their semantic similarity on a scale of 0.0 to 1.0, where 1.0 means they convey the same meaning and 0.0 means completely different meanings.

Answer 1: {answer}
Answer 2: {ground_truth}

Respond with only a decimal number between 0.0 and 1.0:"""

            response = self.ollama_llm.invoke(prompt)
            # Extract numeric score from response
            import re
            score_match = re.search(r'0\.\d+|1\.0|0\.0', response)
            if score_match:
                return float(score_match.group())
            else:
                # Fallback to simple string matching if parsing fails
                return difflib.SequenceMatcher(None, answer.lower(), ground_truth.lower()).ratio()
        except Exception as e:
            print(f"Semantic similarity evaluation failed: {e}")
            return difflib.SequenceMatcher(None, answer.lower(), ground_truth.lower()).ratio()

    def _evaluate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """Use Ollama to evaluate how faithful the answer is to the provided contexts."""
        try:
            context_text = "\n".join(contexts)
            prompt = f"""Evaluate how well this answer is supported by the provided context. Rate on a scale of 0.0 to 1.0, where 1.0 means the answer is fully supported by the context and 0.0 means not supported at all.

Context:
{context_text}

Answer: {answer}

Respond with only a decimal number between 0.0 and 1.0:"""

            response = self.ollama_llm.invoke(prompt)
            # Extract numeric score from response
            import re
            score_match = re.search(r'0\.\d+|1\.0|0\.0', response)
            if score_match:
                return float(score_match.group())
            else:
                return 0.5  # Default middle score if parsing fails
        except Exception as e:
            print(f"Faithfulness evaluation failed: {e}")
            return 0.5

    def _evaluate_context_precision(self, question: str, contexts: List[str]) -> float:
        """Use Ollama to evaluate how relevant the retrieved contexts are to the question."""
        try:
            context_text = "\n".join(contexts)
            prompt = f"""Evaluate how relevant this context is for answering the given question. Rate on a scale of 0.0 to 1.0, where 1.0 means highly relevant and 0.0 means not relevant at all.

Question: {question}

Context:
{context_text}

Respond with only a decimal number between 0.0 and 1.0:"""

            response = self.ollama_llm.invoke(prompt)
            # Extract numeric score from response
            import re
            score_match = re.search(r'0\.\d+|1\.0|0\.0', response)
            if score_match:
                return float(score_match.group())
            else:
                return 0.5  # Default middle score if parsing fails
        except Exception as e:
            print(f"Context precision evaluation failed: {e}")
            return 0.5

    def print_results(self, results):
        """Print evaluation results in a readable format (per-question + averages)."""
        print("\n===== RAG EVALUATION RESULTS =====")

        # Per-question breakdown
        if all(isinstance(results.get(m), (list, np.ndarray)) for m in [
            "answer_relevancy", "faithfulness", "context_precision", "semantic_similarity"
        ]):
            num_items = max(len(results["answer_relevancy"]), len(results["faithfulness"]), len(results["context_precision"]), len(results["semantic_similarity"]))
            for i in range(num_items):
                ar = results["answer_relevancy"][i] if i < len(results["answer_relevancy"]) else None
                fa = results["faithfulness"][i] if i < len(results["faithfulness"]) else None
                cp = results["context_precision"][i] if i < len(results["context_precision"]) else None
                ss = results["semantic_similarity"][i] if i < len(results["semantic_similarity"]) else None
                print(f"Question {i+1}: answer_relevancy={ar:.4f} faithfulness={fa:.4f} context_precision={cp:.4f} semantic_similarity={ss:.4f}")

        print("\n--- Averages ---")
        for metric in results.keys():
            if metric != "model":
                if isinstance(results[metric], (list, np.ndarray)):
                    print(f"{metric}: {np.mean(results[metric]):.4f}")
                else:
                    print(f"{metric}: {results[metric]}")

        print("=================================\n")
        print("Interpretation:")
        print("- Context Precision: How relevant retrieved passages are to the question")
        print("- Context Recall: How much relevant information was retrieved")
        print("- Faithfulness: How well answers stick to the retrieved context")
        print("- Answer Relevancy: How relevant answers are to the questions")
        print("- Harmfulness: Safety check for harmful content in answers")


if __name__ == "__main__":
    # Example usage
    from document_processor import DocumentProcessor

    # Initialize components
    processor = DocumentProcessor(docs_dir="/app/docs")
    evaluator = RAGEvaluator(ground_truth_file="/app/docs/tech/tech_qa_pairs.json")

    # Load documents
    print("Loading documents...")
    docs = processor.load_text_documents(subdirs=["tech"])
    print(f"Loaded {len(docs)} documents")

    # Index documents
    print("Indexing documents for evaluation...")
    evaluator.index_documents(docs)

    # Test questions (from ground truth)
    print("\nPreparing test questions and answers...")
    test_questions = [
        "How do I list files in Linux?",
        "What command is used to change directories in Linux?"
    ]

    # Test with correct answers (matching ground truth)
    correct_answers = [
        "You can use the 'ls' command to list files in Linux.",
        "The 'cd' command is used to change directories in Linux."
    ]

    # Test with incorrect answers
    incorrect_answers = [
        "To list files in Linux, you need to use the 'dir' command, which shows all files in the directory.",
        "The 'changedir' command is used to navigate between directories in Linux systems."
    ]

    # Run evaluations for both correct and incorrect answers
    print("\nRunning evaluation with correct answers...")
    correct_results = evaluator.evaluate(
        questions=test_questions,
        answers=correct_answers,
        model_name="correct-answers"
    )
    evaluator.print_results(correct_results)

    print("\nRunning evaluation with incorrect answers...")
    incorrect_results = evaluator.evaluate(
        questions=test_questions,
        answers=incorrect_answers,
        model_name="incorrect-answers"
    )
    evaluator.print_results(incorrect_results)

    # Compare results
    print("\nMetrics Comparison (Correct vs Incorrect):")
    for metric in correct_results.keys():
        if metric != "model":
            if isinstance(correct_results[metric], (list, np.ndarray)) and isinstance(incorrect_results[metric], (list, np.ndarray)):
                correct_score = np.mean(correct_results[metric])
                incorrect_score = np.mean(incorrect_results[metric])
                diff = correct_score - incorrect_score
                print(f"{metric}: Correct={correct_score:.4f}, Incorrect={incorrect_score:.4f}, Diff={diff:.4f}")
